Checklist â€” Edge-to-Core Comparative Analysis
--------------------------------------------------------------------------------
1. HARNESS & LAB (Validation harness + Docker)
--------------------------------------------------------------------------------
[COMPLETED] Build and run the harness on your x86 laptop (e.g. run_lab.sh or collect_data.sh).
[ ] Build and run the harness on Raspberry Pi 5 (ARM64); e.g. copy repo, run there, or use Fabric to run on Pi over SSH.
[ ] Run on Power10 (ppc64le) or use MOCK_S390X=true for local testing without Power10 access.
[ ] Decide on one image name: either "assurance-harness" (collect_data.sh, fabfile, VS Code task) or "edge-to-core/lab:latest" (docker-compose) and align scripts so build/run commands are consistent.

--------------------------------------------------------------------------------
2. DATA COLLECTION (Per-architecture runs)
--------------------------------------------------------------------------------
[COMPLETED] Single x86 baseline run exists (results/processed_results.json). For full baseline: run collect_data.sh on laptop.
[ ] Collect baseline performance data from Pi 5 / ARM64 (run collect_data.sh on Pi or via Fabric).
[ ] Collect baseline performance data from Power10 or mock s390x.
[ ] Add a way to aggregate results from multiple machines: e.g. script or steps to copy results/x86_*_, results/aarch64_*_, results/ppc64le_* (or s390x) into one place so analysis can read all arches.

--------------------------------------------------------------------------------
3. TIPPING-POINT VISUALIZATIONS (Deliverable)
--------------------------------------------------------------------------------
[COMPLETED] Plotter reads harness JSON (processed_results.json, perf_run_*.json); capability_sweep and efficiency_sweep.
[COMPLETED] Plotter produces graphs: plot_capability.png (threads vs bogo ops/s), plot_efficiency.png (block size vs bandwidth and p99 latency). Use: ./analysis/run_plotter.sh results -o analysis/plots --no-show
[ ] Produce cross-architecture tipping-point plots (e.g. x86 vs ARM64 vs Power10 on the same chart).

--------------------------------------------------------------------------------
4. RESILIENCE COMPARISON (Deliverable)
--------------------------------------------------------------------------------
[ ] Collect MTTR data (mttr_data.csv) on each architecture via SIFI phase of collect_data.sh.
[ ] Add a script or notebook that compares MTTR across architectures (e.g. "Power10 recovered in Xs vs x86 Ys") and optionally plots or summarizes in a resilience log.

--------------------------------------------------------------------------------
5. DOCUMENTATION & DELIVERABLES
--------------------------------------------------------------------------------
[ ] Assurance methodology write-up: short document explaining how your testing methodology ensures reliability of Power Systems (for internship/portfolio).
[ ] (Optional) README: add minimal build/run instructions if you want new users to get started from README alone.

--------------------------------------------------------------------------------
6. NICE-TO-HAVE (Optional)
--------------------------------------------------------------------------------
[ ] Fabric: add a task or doc to run the bench on multiple hosts in sequence (e.g. pi, laptop, Power10 VM) and optionally pull results back to one machine.
[COMPLETED] analysis/requirements.txt (matplotlib); run_plotter.sh uses it.
[ ] Use testbenches/memory_wall.fio in the harness or document when to use it for manual runs.

--------------------------------------------------------------------------------

Summary of main gaps today:
- Cross-arch plots: plotter supports single-arch (or multi-run same arch); combine multiple arches (x86 + ARM64 + Power10) on one chart still to do.
- No aggregation of results from multiple architectures into one dataset.
- No resilience comparison (MTTR across arches) script or report.
- No assurance methodology write-up.
